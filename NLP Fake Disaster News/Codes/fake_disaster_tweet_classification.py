# -*- coding: utf-8 -*-
"""Fake_Disaster_tweet_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DkUir15YAlmOwnBZ6ZyIMIiopqENzp4S

Import the Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""Import the dataset"""

dataset = pd.read_csv("train.csv")
print(dataset['target'].value_counts())

"""Clean the texts"""

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
corpus = []
for i in range (0,7613):
  tweet = re.sub('[^a-zA-Z]', ' ', dataset['text'][i])
  tweet = tweet.lower()
  tweet = tweet.split()
  ps = PorterStemmer()
  tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]
  tweet = ' '.join(tweet)
  corpus.append(tweet)

print(corpus)

"""Creating our bag of words model"""

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=2000)
X = cv.fit_transform(corpus).toarray()
y = dataset.iloc[:, -1].values

"""Splitting our data into training and test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

"""Applying our Machine Learning Model onto our Bag of Words Model"""

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression()
classifier.fit(X_train, y_train)

"""Testing the model against the Test set"""

y_pred = classifier.predict(X_test)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

from sklearn.metrics import confusion_matrix, accuracy_score
print(confusion_matrix(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

from sklearn.model_selection import cross_val_score
accuracy = cross_val_score(estimator=classifier, cv=10, X=X_train, y=y_train)
print('Accuracy :{:.2f}'.format(accuracy.mean()))
print('Stadard Deviation :{:.2f}'.format(accuracy.std()))

"""Predictions for test.csv"""

test_dataset = pd.read_csv('test.csv')
id = test_dataset.iloc[:,0].values
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
corpus = []
for i in range (0,3263):
  tweet = re.sub('[^a-zA-Z]', ' ', test_dataset['text'][i])
  tweet = tweet.lower()
  tweet = tweet.split()
  ps = PorterStemmer()
  tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]
  tweet = ' '.join(tweet)
  corpus.append(tweet)

x_test = cv.transform(corpus).toarray()
y_test = (classifier.predict(x_test))
y_pred = []
for i in y_test:
  y_pred.append(int(round(i)))
y_pred = np.array(y_pred)
print(y_pred)
rows = (np.concatenate((id.reshape(len(id),1), y_pred.reshape(len(y_pred), 1)),1))

import csv
fields = ['id', 'target']
filename = "./submission.csv"
# writing to csv file  
with open(filename, 'w') as csvfile:  
    # creating a csv writer object  
    csvwriter = csv.writer(csvfile)  
        
    # writing the fields  
    csvwriter.writerow(fields)  
        
    # writing the data rows  
    csvwriter.writerows(rows)