{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepNLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgEMXPP7hXT1",
        "colab_type": "text"
      },
      "source": [
        "Each sample in the train and test set has the following information:\n",
        "\n",
        "The text of a tweet\n",
        "A keyword from that tweet (although this may be blank!)\n",
        "The location the tweet was sent from (may also be blank)\n",
        "\n",
        "Task is to predict whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jst9D2ckfnAt",
        "colab_type": "text"
      },
      "source": [
        "Importing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-NKwjrSwxzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNM90L3Ifq9u",
        "colab_type": "text"
      },
      "source": [
        "Importing the train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygTDEgOJw6Tb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a362f455-9c83-4368-b2dc-496271eacac5"
      },
      "source": [
        "dataset = pd.read_csv(\"train.csv\")\n",
        "dataset.head()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO0oTAgPfvEi",
        "colab_type": "text"
      },
      "source": [
        "Cleaning the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk7KH1HuxFER",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "093575b9-5101-44f4-c12b-bc7811889e63"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "corpus = []\n",
        "for i in range (0,7613):\n",
        "  tweet = re.sub('[^a-zA-Z]', ' ', dataset['text'][i])\n",
        "  tweet = tweet.lower()\n",
        "  tweet = tweet.split()\n",
        "  ps = PorterStemmer()\n",
        "  tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]\n",
        "  tweet = ' '.join(tweet)\n",
        "  corpus.append(tweet)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5KfACi1f4k1",
        "colab_type": "text"
      },
      "source": [
        "Splitting the corpus into Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqnbiZJs5zVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "sentence_train, sentence_test, y_train, y_test = train_test_split(corpus, dataset.iloc[:,-1].values, test_size=0.5)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytA5IzcUf-FC",
        "colab_type": "text"
      },
      "source": [
        "Vectorize the text corpus into a list of integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_Nyva_b7HJP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e564a245-e1af-432c-ebb1-b9bc069f87c8"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentence_train)\n",
        "X_train = tokenizer.texts_to_sequences(sentence_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentence_test)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)\n",
        "print(sentence_train[2])\n",
        "print(X_train[2])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11734\n",
            "fedporn feel pain survivor look back period absurd human histori satir indistinguish realiti\n",
            "[3736, 84, 674, 234, 37, 49, 2513, 3737, 402, 496, 3738, 3739, 1935]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBLfOwjHgh3j",
        "colab_type": "text"
      },
      "source": [
        "Pad zeros with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXM4aHj48zyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAbytmWN9CDQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "286c1670-257f-4ca0-9e51-34dfa61f0a87"
      },
      "source": [
        "print(X_train[0, :])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[282   4  64  26   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2Fh2wuggtG_",
        "colab_type": "text"
      },
      "source": [
        "Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEYNXKqD9QhB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "315974a9-b789-465c-bfe7-19e05a643e4d"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, output_dim=50, input_length=maxlen))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_75\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_75 (Embedding)     (None, 100, 50)           586700    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_75 (Glo (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_149 (Dense)            (None, 10)                510       \n",
            "_________________________________________________________________\n",
            "dense_150 (Dense)            (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 587,221\n",
            "Trainable params: 587,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2YsZsoJgyhc",
        "colab_type": "text"
      },
      "source": [
        "Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaBqBTH2_nVi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "015fa4bc-733a-40fc-f037-d36872b59780"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=100)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxdGzGHNg1xD",
        "colab_type": "text"
      },
      "source": [
        "Evaluate it's performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ToSM7uAAYo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2ce729c4-1758-405e-d04d-88c4d96335ae"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.9609\n",
            "Testing Accuracy:  0.7993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5kocFwqEsFt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "21d34a17-a8fe-4577-801c-7caca1475de3"
      },
      "source": [
        "test_dataset = pd.read_csv(\"test.csv\")\n",
        "id = test_dataset.iloc[:,0].values\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "test_corpus = []\n",
        "for i in range (0,3263):\n",
        "  tweet = re.sub('[^a-zA-Z]', ' ', test_dataset['text'][i])\n",
        "  tweet = tweet.lower()\n",
        "  tweet = tweet.split()\n",
        "  ps = PorterStemmer()\n",
        "  tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]\n",
        "  tweet = ' '.join(tweet)\n",
        "  test_corpus.append(tweet)\n",
        "\n",
        "val_test = tokenizer.texts_to_sequences(test_corpus)\n",
        "val_test = pad_sequences(val_test, padding='post', maxlen=maxlen)\n",
        "result = (model.predict(val_test))\n",
        "print(result)\n",
        "y_pred = []\n",
        "for i in result:\n",
        "  y_pred.append(int(round(i[0])))\n",
        "y_pred = np.array(y_pred)\n",
        "print(y_pred)\n",
        "rows = (np.concatenate((id.reshape(len(id),1), y_pred.reshape(len(y_pred), 1)),1))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[[0.8710285 ]\n",
            " [0.18318215]\n",
            " [0.9353117 ]\n",
            " ...\n",
            " [0.8419656 ]\n",
            " [0.9386988 ]\n",
            " [0.5403851 ]]\n",
            "[1 0 1 ... 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DgpV55GIKi1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "898aaa91-6bf0-4a6c-bdf0-81cdcccff24d"
      },
      "source": [
        "print(len(rows))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9Za5cNsIFD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "fields = ['id', 'target']\n",
        "filename = \"./submission.csv\"\n",
        "# writing to csv file  \n",
        "with open(filename, 'w') as csvfile:  \n",
        "    # creating a csv writer object  \n",
        "    csvwriter = csv.writer(csvfile)  \n",
        "        \n",
        "    # writing the fields  \n",
        "    csvwriter.writerow(fields)  \n",
        "        \n",
        "    # writing the data rows  \n",
        "    csvwriter.writerows(rows) "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sel93ctzJCm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
        "    model = Sequential()\n",
        "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(10, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfyA6eWsJkbK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ca2bb001-55b2-469d-a1c0-b0e4891d0fcf"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Main settings\n",
        "epochs = 10\n",
        "embedding_dim = 50\n",
        "maxlen = 100\n",
        "\n",
        "# Parameter grid for grid search\n",
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                kernel_size=[3, 5, 7],\n",
        "                vocab_size=[vocab_size],\n",
        "                embedding_dim=[embedding_dim],\n",
        "                maxlen=[maxlen])\n",
        "model = KerasClassifier(build_fn=create_model,\n",
        "                      epochs=epochs, batch_size=10,\n",
        "                      verbose=False)\n",
        "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
        "                        cv=4, verbose=1, n_iter=5)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate testing set\n",
        "test_accuracy = grid.score(X_test, y_test)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 10.1min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YIgR_WSMDNg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bd10dd72-b9f6-401e-bf84-7375f1ee5116"
      },
      "source": [
        "print( grid_result.best_score_, grid_result.best_params_, test_accuracy)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7506560981273651 {'vocab_size': 11734, 'num_filters': 32, 'maxlen': 100, 'kernel_size': 7, 'embedding_dim': 50} 0.7743630409240723\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}