# -*- coding: utf-8 -*-
"""DeepNLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ha6diLmPNYyHcvGqKBLfTWJ307OlTFG2

Each sample in the train and test set has the following information:

The text of a tweet, a keyword from that tweet (although this may be blank!),the location the tweet was sent from (may also be blank)

Task is to predict whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.

Importing the Libraries
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras

"""Importing the train dataset"""

dataset = pd.read_csv("train.csv")
dataset.head()

"""Cleaning the text"""

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
corpus = []
for i in range (0,7613):
  tweet = re.sub('[^a-zA-Z]', ' ', dataset['text'][i])
  tweet = tweet.lower()
  tweet = tweet.split()
  ps = PorterStemmer()
  tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]
  tweet = ' '.join(tweet)
  corpus.append(tweet)

"""Splitting the corpus into Train and Test sets"""

from sklearn.model_selection import train_test_split
sentence_train, sentence_test, y_train, y_test = train_test_split(corpus, dataset.iloc[:,-1].values, test_size=0.1)

"""Vectorize the text corpus into a list of integers"""

from keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(sentence_train)
X_train = tokenizer.texts_to_sequences(sentence_train)
X_test = tokenizer.texts_to_sequences(sentence_test)
vocab_size = len(tokenizer.word_index) + 1
print(vocab_size)
print(sentence_train[2])
print(X_train[2])

"""Pad zeros with Keras"""

from keras.preprocessing.sequence import pad_sequences

maxlen = 100
X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

print(X_train[0, :])

"""Build the model"""

from keras.models import Sequential
from keras import layers
from keras.optimizers import Adam
opt = Adam(lr=0.0004, decay=0.0004/10)

model = Sequential()
model.add(layers.Embedding(input_dim=vocab_size, output_dim=50, input_length=maxlen))
model.add(layers.GlobalMaxPooling1D())
model.add(layers.Dense(10, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer=opt,
              loss='binary_crossentropy',
              metrics=['accuracy'])
model.summary()

"""Fit the model"""

history = model.fit(X_train, y_train,
                    epochs=10,
                    verbose=False,
                    validation_data=(X_test, y_test),
                    batch_size=100)

"""Evaluate it's performance"""

loss, accuracy = model.evaluate(X_train, y_train, verbose=False)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

test_dataset = pd.read_csv("test.csv")
id = test_dataset.iloc[:,0].values

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
test_corpus = []
for i in range (0,3263):
  tweet = re.sub('[^a-zA-Z]', ' ', test_dataset['text'][i])
  tweet = tweet.lower()
  tweet = tweet.split()
  ps = PorterStemmer()
  tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]
  tweet = ' '.join(tweet)
  test_corpus.append(tweet)

val_test = tokenizer.texts_to_sequences(test_corpus)
val_test = pad_sequences(val_test, padding='post', maxlen=maxlen)
result = (model.predict(val_test))
print(result)
y_pred = []
for i in result:
  y_pred.append(int(round(i[0])))
y_pred = np.array(y_pred)
print(y_pred)
rows = (np.concatenate((id.reshape(len(id),1), y_pred.reshape(len(y_pred), 1)),1))

print(len(rows))

import csv
fields = ['id', 'target']
filename = "./submission.csv"
# writing to csv file  
with open(filename, 'w') as csvfile:  
    # creating a csv writer object  
    csvwriter = csv.writer(csvfile)  
        
    # writing the fields  
    csvwriter.writerow(fields)  
        
    # writing the data rows  
    csvwriter.writerows(rows)

def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):
    model = Sequential()
    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))
    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))
    model.add(layers.GlobalMaxPooling1D())
    model.add(layers.Dense(10, activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model

from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import RandomizedSearchCV

# Main settings
epochs = 10
embedding_dim = 50
maxlen = 100

# Parameter grid for grid search
param_grid = dict(num_filters=[32, 64, 128],
                kernel_size=[3, 5, 7],
                vocab_size=[vocab_size],
                embedding_dim=[embedding_dim],
                maxlen=[maxlen])
model = KerasClassifier(build_fn=create_model,
                      epochs=epochs, batch_size=10,
                      verbose=False)
grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,
                        cv=4, verbose=1, n_iter=5)
grid_result = grid.fit(X_train, y_train)

# Evaluate testing set
test_accuracy = grid.score(X_test, y_test)

print( grid_result.best_score_, grid_result.best_params_, test_accuracy)