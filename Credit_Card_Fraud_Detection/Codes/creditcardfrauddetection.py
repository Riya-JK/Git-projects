# -*- coding: utf-8 -*-
"""Copy of CreditCardFraudDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b4OepSmUfK6uaMXUmG0Ij9e9LQXDRLkk

**Credit Card Fraud Detection**

The dataset used here has been downloaded from kaggle and presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.

Import the Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""Import the Dataset"""

dataset = pd.read_csv('creditcard.csv')
print(dataset["Class"].value_counts())
#Resampling the dataset by considering only the first 20,000 transactions
dataset_20000 = dataset[:20000]
print(dataset_20000["Class"].value_counts())
X = dataset_20000.iloc[:, :-1].values
y = dataset_20000.iloc[:, -1].values
y = y.reshape(len(y),1)

"""Taking Care of the missing values"""

from sklearn.impute import SimpleImputer
imputer_X = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer_Y = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
imputer_X.fit(X)
imputer_Y.fit(y)
X = imputer_X.transform(X)
y = imputer_Y.transform(y)

"""Splitting the dataset"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
print(X_train.shape, X_test.shape)

"""Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)

"""Finding out the best K value Using the K-Fold cross validator"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
cv_score = []
for i in range(1,6):
  KNN = KNeighborsClassifier(n_neighbors=i)
  validator = cross_val_score(estimator=KNN, X=X_train, y=y_train, cv=5, scoring='recall')
  cv_score.append(validator.mean())

plt.plot(range(1,6), cv_score, color='blue')
plt.xlabel("No. of neighbors")
plt.ylabel("CV Score")
plt.show()

"""From the graph it is clear that the best CV Score is achieved for K = 1.

Training the Model
"""

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 1)
classifier.fit(X_train, y_train)

"""Testing the Model"""

y_pred = classifier.predict(sc.transform(X_test))
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test), 1)), axis=1))

"""Confusion Matrix and Accuracy Score"""

from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score
cf = confusion_matrix(y_test, y_pred)
ac = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
rs = recall_score(y_test, y_pred)
print(cf)
print('Accuracy Score score: {0:0.2f}'.format(
      ac*100))
print('F1 score: {0:0.2f}'.format(
      f1*100))
print('Recall score: {0:0.2f}'.format(
      rs*100))

"""As this is a highly imbalanced dataset we cannot rely on the accuracy score alone and will make use of other metrics such as Average Precision Score and F1 Score.

Computing the average precision score
"""

from sklearn.metrics import average_precision_score
average_precision = average_precision_score(y_test, y_pred)
print('Average precision-recall score: {0:0.2f}'.format(
      average_precision*100))

"""Plotting the precision recall curve"""

from sklearn.metrics import precision_recall_curve
from sklearn.metrics import plot_precision_recall_curve
disp = plot_precision_recall_curve(classifier, X_test, y_test)
disp.ax_.set_title('2-class Precision-Recall curve: '
                   'AP={0:0.2f}'.format(average_precision))